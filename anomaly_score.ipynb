{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import numpy\n",
    "import os\n",
    "import time\n",
    "\n",
    "from nupic.algorithms.sdr_classifier_factory import SDRClassifierFactory\n",
    "from nupic.algorithms.spatial_pooler import SpatialPooler\n",
    "from nupic.algorithms.temporal_memory import TemporalMemory\n",
    "from nupic.algorithms.anomaly import computeRawAnomalyScore\n",
    "from nupic.algorithms.anomaly import Anomaly\n",
    "from nupic.encoders.date import DateEncoder\n",
    "from nupic.encoders.adaptive_scalar import AdaptiveScalarEncoder\n",
    "\n",
    "from model_params import MODEL_PARAMS\n",
    "\n",
    "_NUM_RECORDS = 12000\n",
    "\n",
    "_INPUT_FILE_PATH = \"data.csv\"\n",
    "\n",
    "def runHotgym(numRecords):\n",
    "    modelParams = MODEL_PARAMS[\"modelParams\"]\n",
    "    enParams = modelParams[\"sensorParams\"][\"encoders\"]\n",
    "    spParams = modelParams[\"spParams\"]\n",
    "    tmParams = modelParams[\"tmParams\"]\n",
    "\n",
    "    scalarEncoder = AdaptiveScalarEncoder(enParams[\"acceleration\"][\"w\"], n = enParams[\"acceleration\"][\"n\"])\n",
    "\n",
    "    encodingWidth = scalarEncoder.getWidth()\n",
    "\n",
    "    sp = SpatialPooler(\n",
    "    inputDimensions=(encodingWidth),\n",
    "    columnDimensions=(spParams[\"columnCount\"]),\n",
    "    potentialPct=spParams[\"potentialPct\"],\n",
    "    globalInhibition=spParams[\"globalInhibition\"],\n",
    "    #localAreaDensity=spParams[\"localAreaDensity\"],\n",
    "    numActiveColumnsPerInhArea=spParams[\"numActiveColumnsPerInhArea\"],\n",
    "    synPermInactiveDec=spParams[\"synPermInactiveDec\"],\n",
    "    synPermActiveInc=spParams[\"synPermActiveInc\"],\n",
    "    synPermConnected=spParams[\"synPermConnected\"],\n",
    "    boostStrength=spParams[\"boostStrength\"],\n",
    "    seed=spParams[\"seed\"],\n",
    "    wrapAround=False\n",
    "  )\n",
    "\n",
    "    tm = TemporalMemory(\n",
    "    columnDimensions=(tmParams[\"columnCount\"],),\n",
    "    cellsPerColumn=tmParams[\"cellsPerColumn\"],\n",
    "    activationThreshold=tmParams[\"activationThreshold\"],\n",
    "    initialPermanence=tmParams[\"initialPerm\"],\n",
    "    connectedPermanence=spParams[\"synPermConnected\"],\n",
    "    minThreshold=tmParams[\"minThreshold\"],\n",
    "    maxNewSynapseCount=tmParams[\"newSynapseCount\"],\n",
    "    permanenceIncrement=tmParams[\"permanenceInc\"],\n",
    "    permanenceDecrement=tmParams[\"permanenceDec\"],\n",
    "    predictedSegmentDecrement=0.0,\n",
    "    maxSegmentsPerCell=tmParams[\"maxSegmentsPerCell\"],\n",
    "    maxSynapsesPerSegment=tmParams[\"maxSynapsesPerSegment\"],\n",
    "    seed=tmParams[\"seed\"]\n",
    "  )\n",
    "\n",
    "    an = Anomaly(slidingWindowSize=1, mode='likelihood')\n",
    "    \n",
    "    classifier = SDRClassifierFactory.create()\n",
    "    \n",
    "    results = []\n",
    "    anomalyScore = []\n",
    "    \n",
    "    global tp_time\n",
    "    tp_time = []\n",
    "    \n",
    "    with open(_INPUT_FILE_PATH, \"r\") as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        headers = reader.next()\n",
    "        reader.next()\n",
    "        reader.next()\n",
    "\n",
    "        ema = 0\n",
    "        FORGETING_FACTOR = 1\n",
    "        \n",
    "        for count, record in enumerate(reader):\n",
    "            if count >= numRecords: \n",
    "                print \"Finished\"\n",
    "                break\n",
    "\n",
    "            start = time.time()   \n",
    "            print count\n",
    "            \n",
    "            # Convert input data value string into float.\n",
    "            acceleration = float(record[0])\n",
    "\n",
    "\n",
    "            # To encode, we need to provide zero-filled numpy arrays for the encoders\n",
    "            # to populate.\n",
    "            consumptionBits = numpy.zeros(scalarEncoder.getWidth())\n",
    "\n",
    "            # Now we call the encoders create bit representations for each value.\n",
    "            scalarEncoder.encodeIntoArray(acceleration, consumptionBits)\n",
    "\n",
    "            # Concatenate all these encodings into one large encoding for Spatial\n",
    "            # Pooling.\n",
    "            encoding = consumptionBits\n",
    "            \n",
    "            print (\"ENC time:{0}\".format(time.time() - start)) + \"[sec]\"\n",
    "            start = time.time()\n",
    "\n",
    "            # Create an array to represent active columns, all initially zero. This\n",
    "            # will be populated by the compute method below. It must have the same\n",
    "            # dimensions as the Spatial Pooler.\n",
    "            activeColumns = numpy.zeros(spParams[\"columnCount\"])\n",
    "\n",
    "            # Execute Spatial Pooling algorithm over input space.\n",
    "            sp.compute(encoding, True, activeColumns)\n",
    "            activeColumnIndices = numpy.nonzero(activeColumns)[0]\n",
    "            \n",
    "            print (\"SP time:{0}\".format(time.time() - start)) + \"[sec]\"\n",
    "            start = time.time()\n",
    "            \n",
    "            if count > 1 : \n",
    "                #print tm.numberOfCells()\n",
    "                #print tm.cellsPerColumn\n",
    "                #print predictiveCells\n",
    "                #print activeColumnIndices\n",
    "                prevPredictedColumns = numpy.array(predictiveCells) / tm.cellsPerColumn\n",
    "                anomalyScore.append(an.compute(activeColumnIndices, prevPredictedColumns, inputValue= acceleration, timestamp=None))\n",
    "\n",
    "            # Execute Temporal Memory algorithm over active mini-columns.\n",
    "            tm.compute(activeColumnIndices, learn=True)\n",
    "            print \"No of active segments:{0}\".format(len(tm.getActiveSegments()))\n",
    "            \n",
    "            print (\"TM time:{0}\".format(time.time() - start)) + \"[sec]\"\n",
    "            tp_time.append(time.time() - start)\n",
    "            start = time.time()\n",
    "\n",
    "            activeCells = tm.getActiveCells()\n",
    "             \n",
    "            predictiveCells = tm.getPredictiveCells()\n",
    "\n",
    "            # Get the bucket info for this input value for classification.\n",
    "            bucketIdx = scalarEncoder.getBucketIndices(acceleration)[0]\n",
    "\n",
    "            # Run classifier to translate active cells back to scalar value.\n",
    "            classifierResult = classifier.compute(\n",
    "              recordNum=count,\n",
    "              patternNZ=activeCells,\n",
    "              classification={\n",
    "              \"bucketIdx\": bucketIdx,\n",
    "              \"actValue\": acceleration\n",
    "            },\n",
    "            learn=True,\n",
    "            infer=True\n",
    "            )\n",
    "\n",
    "            print (\"Classifier time:{0}\".format(time.time() - start)) + \"[sec]\"\n",
    "            start = time.time()\n",
    "            \n",
    "            # Print the best prediction for 1 step out.\n",
    "            oneStepConfidence, oneStep = sorted(\n",
    "            zip(classifierResult[1], classifierResult[\"actualValues\"]),\n",
    "            reverse=True\n",
    "            )[0]\n",
    "            #print(\"1-step: {:16} ({:4.4}%)\".format(oneStep, oneStepConfidence * 100))\n",
    "            results.append([oneStep, oneStepConfidence * 100, None, None])\n",
    "            \n",
    "            print (\"Store data time:{0}\".format(time.time() - start)) + \"[sec]\"\n",
    "\n",
    "    return results, anomalyScore\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    results, anomalyScore = runHotgym(_NUM_RECORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_file, show, output_notebook\n",
    "output_notebook()\n",
    "\n",
    "with open('tp_time.dump', 'wb') as f:\n",
    "    pickle.dump(tp_time, f)\n",
    "    \n",
    "with open('tp_time.dump', 'rb') as f:\n",
    "    tp_time = pickle.load(f)\n",
    "    \n",
    "t= range(len(tp_time))\n",
    "    \n",
    "pp = figure(tools='xwheel_zoom,xpan',\n",
    "title=\"\",\n",
    "x_axis_label='n',\n",
    "y_axis_label='time[sec]')\n",
    "pp.line(t, tp_time,legend=\"TP time\", line_width=1, line_color = \"blue\")\n",
    "show(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "\n",
    "with open('results.dump', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "    \n",
    "with open('anomalyScore.dump', 'rb') as f:\n",
    "    anomalyScore = pickle.load(f)\n",
    "    \n",
    "_INPUT_FILE_PATH = \"data.csv\"\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_file, show, output_notebook\n",
    "output_notebook()\n",
    "\n",
    "predicted =[]\n",
    "actual =[]\n",
    "anomaly =[]\n",
    "\n",
    "for i in range(len(results) -2):\n",
    "    predicted.append(results[i][0])\n",
    "    anomaly.append(anomalyScore[i] * 400)\n",
    "\n",
    "with open(_INPUT_FILE_PATH, \"r\") as fin:\n",
    "    reader = csv.reader(fin)\n",
    "    headers = reader.next()\n",
    "    reader.next()\n",
    "    reader.next()\n",
    "    \n",
    "    for count, record in enumerate(reader):\n",
    "        actual.append(float(record[0]))\n",
    "\n",
    "t= range(len(predicted)-1)\n",
    "        \n",
    "pp = figure(tools='xwheel_zoom,xpan',\n",
    "title=\"\",\n",
    "x_axis_label='Time[sec]',\n",
    "y_axis_label='Acceleration')\n",
    "pp.line(t, actual[1:12000],legend=\"Acceleration\", line_width=1, line_color = \"blue\")\n",
    "#pp.line(t, predicted[:11999],legend=\"Y\", line_width=1, line_color = \"green\")\n",
    "pp.line(t, anomaly[:11999] ,legend=\"Anomaly score\", line_width=1, line_color = \"red\")\n",
    "show(pp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
